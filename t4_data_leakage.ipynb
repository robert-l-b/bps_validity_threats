{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T4 Data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pm4py\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data to read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Helpdesk.xes',\n",
       " 'BPI_Challenge_2012_W_Two_TS.xes',\n",
       " 'PurchasingExample.xes',\n",
       " 'BPI_Challenge_2017_W_Two_TS.xes',\n",
       " 'ConsultaDataMining201618.xes',\n",
       " 'cvs_pharmacy.xes']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir_path = os.path.join('..', 'data', 'full_logs')\n",
    "\n",
    "os.listdir(log_dir_path)\n",
    "# path_to_log = os.path.join(path_to_logs, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = [\n",
    "    'BPI_Challenge_2012_W_Two_TS.xes',\n",
    "    'PurchasingExample.xes',\n",
    "    'BPI_Challenge_2017_W_Two_TS.xes',\n",
    "    'ConsultaDataMining201618.xes',\n",
    "    'cvs_pharmacy.xes'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_lifecycle(log):\n",
    "\n",
    "    # Sample data setup\n",
    "    # log = pd.DataFrame( ... )\n",
    "\n",
    "    # Step 0: Filter for relevant lifecycle transitions (\"start\", \"complete\", \"completed\")\n",
    "    filtered_log = log[log[\"lifecycle:transition\"].isin([\"start\", \"complete\", \"completed\"])].copy()\n",
    "\n",
    "    # Step 1: Create a shifted version of the filtered dataframe to compare consecutive rows\n",
    "    filtered_shifted = filtered_log.shift(-1)\n",
    "\n",
    "    # Step 2: Identify pairs where the first row has \"start\" and the second has \"complete\" or \"completed\"\n",
    "    mask = (\n",
    "        (filtered_log[\"lifecycle:transition\"] == \"start\") &\n",
    "        (filtered_shifted[\"lifecycle:transition\"].isin([\"complete\", \"completed\"]))\n",
    "    )\n",
    "\n",
    "    # Step 3: Ensure all columns except \"lifecycle:transition\" and \"time:timestamp\" are the same for the pair\n",
    "    columns_to_compare = filtered_log.columns.difference([\"lifecycle:transition\", \"time:timestamp\"])\n",
    "    same_values_mask = (\n",
    "        filtered_log[columns_to_compare].eq(filtered_shifted[columns_to_compare]) |\n",
    "        (filtered_log[columns_to_compare].isna() & filtered_shifted[columns_to_compare].isna())\n",
    "    ).all(axis=1)\n",
    "\n",
    "    # Step 4: Combine the two conditions (lifecycle transition and matching values in other columns)\n",
    "    valid_pairs_mask = mask & same_values_mask\n",
    "\n",
    "    # Step 5: Create a new column \"event_id\" and assign unique IDs to the valid pairs\n",
    "    filtered_log[\"event_id\"] = None  # Initialize with None\n",
    "\n",
    "    # Assign unique IDs to each valid pair\n",
    "    event_id_counter = 1\n",
    "    for idx in filtered_log[valid_pairs_mask].index:\n",
    "        filtered_log.at[idx, \"event_id\"] = event_id_counter  # Assign to \"start\" row\n",
    "        filtered_log.at[idx + 1, \"event_id\"] = event_id_counter  # Assign to \"complete\" row\n",
    "        event_id_counter += 1\n",
    "\n",
    "    # The dataframe 'log' now has a unique 'event_id' for each valid consecutive \"start\" and \"complete\n",
    "\n",
    "    return filtered_log\n",
    "\n",
    "\n",
    "def convert_timestamps(log):\n",
    "\n",
    "    log = group_lifecycle(log)\n",
    "\n",
    "    # slit logs in start and complete\n",
    "    log_start = log[log['lifecycle:transition'] == 'start']\n",
    "    log_compl = log[log['lifecycle:transition'] == 'complete']\n",
    "\n",
    "\n",
    "    # rename and drop column\n",
    "    log_start = log_start.rename(columns = {'time:timestamp': 'start_timestamp'})\n",
    "    log_compl = log_compl.rename(columns = {'time:timestamp': 'end_timestamp'})\n",
    "\n",
    "    # filter for only necessary columns\n",
    "    key_cols = ['case:concept:name', 'concept:name', 'event_id']\n",
    "    filter_cols = key_cols.copy()\n",
    "    filter_cols.append('start_timestamp')\n",
    "    log_start = log_start[filter_cols]\n",
    "    \n",
    "    # displaying\n",
    "    # display(log_compl.shape, log_start.shape)\n",
    "    log_merged = log_compl.merge(log_start, left_on=key_cols, right_on=key_cols, how='left')\n",
    "    # display(log_merged[log_merged['case:concept:name'] == str(13)][['concept:name', 'case:concept:name', 'start_timestamp', 'end_timestamp']])  \n",
    "\n",
    "    # return log_start, log_compl\n",
    "    return log_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_leakage_share(logs, log_name, print_bool=False):\n",
    "    \n",
    "    extraction_key = 'starting'\n",
    "    \n",
    "    if extraction_key not in logs[log_name]['splits'].keys():\n",
    "        extraction_key = 'full'\n",
    "\n",
    "    train_log = logs[log_name]['splits'][extraction_key]['regular']['train']\n",
    "    test_log = logs[log_name]['splits'][extraction_key]['regular']['test']\n",
    "\n",
    "    timestamp = 'start_timestamp'\n",
    "    # Step 1: Get the earliest timestamp from the test_log\n",
    "    earliest_test_timestamp = test_log[timestamp].min()\n",
    "\n",
    "    # Step 2: Count events in train_log with a timestamp past the earliest timestamp in test_log\n",
    "    train_events_past_test_start = train_log[train_log[timestamp] > earliest_test_timestamp]\n",
    "    num_train_events_past_test_start = len(train_events_past_test_start)\n",
    "\n",
    "    # Step 3: Count the total number of events in logs\n",
    "    num_train_events = len(train_log)\n",
    "    num_test_events = len(test_log)\n",
    "\n",
    "    share_of_train = num_train_events_past_test_start/num_train_events\n",
    "    share_of_test  = num_train_events_past_test_start/num_test_events\n",
    "\n",
    "    logs[log_name]['leakage'] = {\n",
    "        'train':share_of_train,\n",
    "        'test': share_of_test\n",
    "        }\n",
    "\n",
    "    if print_bool:\n",
    "        # Print results\n",
    "        print(\"Earliest timestamp in test_log:\", earliest_test_timestamp)\n",
    "        print(\"Number of events in train_log past this timestamp:\", num_train_events_past_test_start)\n",
    "        print(\"Total number of events in train_log:\", num_train_events)\n",
    "        print(\"Total number of events in test_log:\", num_test_events)\n",
    "\n",
    "        print('Share of intersecting events:')\n",
    "        print(' - share of train log: ', share_of_train )\n",
    "        print(' - share of test log: ', share_of_test)\n",
    "\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overalap calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 8616/8616 [00:03<00:00, 2521.25it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 608/608 [00:00<00:00, 2011.38it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 30276/30276 [00:10<00:00, 2955.77it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 954/954 [00:00<00:00, 3258.74it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 10000/10000 [00:05<00:00, 1921.76it/s]\n"
     ]
    }
   ],
   "source": [
    "logs = {}\n",
    "target_dir_path = ''\n",
    "criterium_value = 80\n",
    "\n",
    "for log_name in log_names:\n",
    "    path_to_log = os.path.join(log_dir_path, log_name)  \n",
    "    \n",
    "    log = pm4py.read_xes(path_to_log)\n",
    "    log = convert_timestamps(log)\n",
    "    \n",
    "    logs = extract_split_logs(logs, log, log_name, target_dir_path, criterium_value, extract=False, cut_dates=None, write_out_logs = False)\n",
    "    logs = get_data_leakage_share(logs, log_name, print_bool=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overlap for running example from T3\n",
    "log = logs['BPI_Challenge_2012_W_Two_TS.xes']['full_log']\n",
    "log_name = 'example_t3'\n",
    "extract = True\n",
    "start_time = pd.to_datetime('2011-11-1 00:00:00').tz_localize('UTC')\n",
    "end_time   = pd.to_datetime('2012-01-31 23:59:59').tz_localize('UTC')\n",
    "cut_dates = [start_time, end_time]\n",
    "logs = extract_split_logs(logs, log, log_name, target_dir_path, criterium_value, extract=extract, cut_dates=cut_dates, write_out_logs = False)\n",
    "logs = get_data_leakage_share(logs, log_name, print_bool=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BPI_Challenge_2012_W_Two_TS.xes\n",
      "{'train': 0.08778355879292404, 'test': 0.3748666903661571}\n",
      "\n",
      " PurchasingExample.xes\n",
      "{'train': 0.10483356714704757, 'test': 0.6431924882629108}\n",
      "\n",
      " BPI_Challenge_2017_W_Two_TS.xes\n",
      "{'train': 0.028004197827726042, 'test': 0.12335068665290369}\n",
      "\n",
      " ConsultaDataMining201618.xes\n",
      "{'train': 0.06735299249694643, 'test': 0.3341991341991342}\n",
      "\n",
      " cvs_pharmacy.xes\n",
      "{'train': 0.11484507840544114, 'test': 0.4597712016639879}\n",
      "\n",
      " example_t3\n",
      "{'train': 0.15385730858468677, 'test': 0.598589562764457}\n"
     ]
    }
   ],
   "source": [
    "for log_name in logs.keys():\n",
    "    print('\\n', log_name)\n",
    "    print(logs[log_name]['leakage'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bps_evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
